{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6971474,"sourceType":"datasetVersion","datasetId":4005497},{"sourceId":6971502,"sourceType":"datasetVersion","datasetId":4005518},{"sourceId":11944813,"sourceType":"datasetVersion","datasetId":7509168}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pytorch==1.12 torchvision torchaudio cudatoolkit==11.3\n!pip install timm==0.6.5\n!pip install pytorch-msssim\n!pip install opencv-python==4.5.5.62\n!pip install tqdm tensorboard tensorboardx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:53:03.692985Z","iopub.execute_input":"2025-05-24T16:53:03.693673Z","iopub.status.idle":"2025-05-24T16:54:35.400622Z","shell.execute_reply.started":"2025-05-24T16:53:03.693627Z","shell.execute_reply":"2025-05-24T16:54:35.399915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport pandas as pd\n\n#from utils import AverageMeter, CosineScheduler, pad_img\n#from datasets import PairLoader\n#from models import *\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision.utils import save_image\nimport subprocess\nimport time\nimport numpy as np\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:10:47.990485Z","iopub.execute_input":"2025-05-25T11:10:47.991230Z","iopub.status.idle":"2025-05-25T11:11:12.093185Z","shell.execute_reply.started":"2025-05-25T11:10:47.991203Z","shell.execute_reply":"2025-05-25T11:11:12.092615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#--------------------------------------------------UTILS FOLDER--------------------------------------------------------------------\n\nimport math\nimport torch\n\nfrom timm.scheduler.scheduler import Scheduler\n\n\nclass CosineScheduler(Scheduler):\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 param_name: str,\n                 t_max: int,\n                 value_min: float = 0.,\n                 warmup_t=0,\n                 const_t=0,\n                 initialize=True) -> None:\n        super().__init__(\n            optimizer, param_group_field=param_name, initialize=initialize)\n\n        assert t_max > 0\n        assert value_min >= 0\n        assert warmup_t >= 0\n        assert const_t >= 0\n        \n        #self.t_max = t_max \n\n        self.cosine_t = t_max - warmup_t - const_t\n        self.value_min = value_min\n        self.warmup_t = warmup_t\n        self.const_t = const_t\n\n        if self.warmup_t:\n            self.warmup_steps = [(v - value_min) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.value_min)\n        else:\n            self.warmup_steps = []\n\n    def _get_value(self, t):\n        if t < self.warmup_t:\n            values = [self.value_min + t * s for s in self.warmup_steps]\n        elif t < self.warmup_t + self.const_t:\n            values = self.base_values\n        else:\n            t = t - self.warmup_t - self.const_t\n\n            value_max_values = [v for v in self.base_values]\n\n            values = [\n                self.value_min + 0.5 * (value_max - self.value_min) * (1 + math.cos(math.pi * t / self.cosine_t))\n                for value_max in value_max_values\n            ]\n\n        return values\n\n    def get_epoch_values(self, epoch: int):\n        return self._get_value(epoch)\n\n\n    def _get_lr(self):\n        #Compute the learning rate for the current epoch.\n        if self.last_epoch < self.warmup_t:\n            # Linear warmup phase\n            lr = self.value_min + (self.base_lrs[0] - self.value_min) * (self.last_epoch / self.warmup_t)\n        elif self.last_epoch < self.warmup_t + self.const_t:\n            # Constant learning rate phase\n            lr = self.base_lrs[0]\n        else:\n            # Cosine annealing phase\n            progress = (self.last_epoch - self.warmup_t - self.const_t) / (self.t_max - self.warmup_t - self.const_t)\n        lr = self.value_min + 0.5 * (self.base_lrs[0] - self.value_min) * (1 + torch.cos(progress * 3.141592653589793))\n    \n        return [lr for _ in self.optimizer.param_groups]\n\n\n\n\nclass AverageMeter(object):\n\tdef __init__(self):\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.val = 0\n\t\tself.avg = 0\n\t\tself.sum = 0\n\t\tself.count = 0\n\n\tdef update(self, val, n=1):\n\t\tself.val = val\n\t\tself.sum += val * n\n\t\tself.count += n\n\t\tself.avg = self.sum / self.count\n\n\ndef pad_img(x, patch_size):\n\t_, _, h, w = x.size()\n\tmod_pad_h = (patch_size - h % patch_size) % patch_size\n\tmod_pad_w = (patch_size - w % patch_size) % patch_size\n\tx = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n\treturn x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:12.094350Z","iopub.execute_input":"2025-05-25T11:11:12.095101Z","iopub.status.idle":"2025-05-25T11:11:15.776308Z","shell.execute_reply.started":"2025-05-25T11:11:12.095080Z","shell.execute_reply":"2025-05-25T11:11:15.775614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#------------------------------------------------------DATASETS FOLDER---------------------------------------------------------------\nfrom torch.utils.data import Dataset\nimport cv2\nimport random\n\ndef read_img(filename, to_float=True):\n\timg = cv2.imread(filename)\n\tif to_float: img = img.astype('float32') / 255.0\n\treturn img[:, :, ::-1]\n\n\ndef hwc_to_chw(img):\n\treturn np.transpose(img, axes=[2, 0, 1]).copy()\n\n\ndef augment(imgs=[], size=256, edge_decay=0., data_augment=True):\n\tH, W, _ = imgs[0].shape\n\tHc, Wc = [size, size]\n\n\t# simple re-weight for the edge\n\tif random.random() < Hc / H * edge_decay:\n\t\tHs = 0 if random.randint(0, 1) == 0 else H - Hc\n\telse:\n\t\tHs = random.randint(0, H-Hc)\n\n\tif random.random() < Wc / W * edge_decay:\n\t\tWs = 0 if random.randint(0, 1) == 0 else W - Wc\n\telse:\n\t\tWs = random.randint(0, W-Wc)\n\n\tfor i in range(len(imgs)):\n\t\timgs[i] = imgs[i][Hs:(Hs+Hc), Ws:(Ws+Wc), :]\n\n\tif data_augment:\n\t\t# horizontal flip\n\t\tif random.randint(0, 1) == 1:\n\t\t\tfor i in range(len(imgs)):\n\t\t\t\timgs[i] = np.flip(imgs[i], axis=1)\n\n\t\t# bad data augmentations for outdoor dehazing\n\t\trot_deg = random.randint(0, 3)\n\t\tfor i in range(len(imgs)):\n\t\t\timgs[i] = np.rot90(imgs[i], rot_deg, (0, 1))\n\t\t\t\n\treturn imgs\n\n\ndef align(imgs=[], size=256):\n\tH, W, _ = imgs[0].shape\n\tHc, Wc = [size, size]\n\n\tHs = (H - Hc) // 2\n\tWs = (W - Wc) // 2\n\tfor i in range(len(imgs)):\n\t\timgs[i] = imgs[i][Hs:(Hs+Hc), Ws:(Ws+Wc), :]\n\n\treturn imgs\n\n\n\n\n\nclass PairLoader(Dataset):\n\tdef __init__(self, root_dir, mode, size=256, edge_decay=0, data_augment=True, cache_memory=False):\n\t\tassert mode in ['train', 'valid', 'test']\n\n\t\tself.mode = mode\n\t\tself.size = size\n\t\tself.edge_decay = edge_decay\n\t\tself.data_augment = data_augment\n\n\t\tself.root_dir = root_dir\n\t\tself.img_names = sorted(os.listdir(os.path.join(self.root_dir, 'GT')))\n\t\tself.img_num = len(self.img_names)\n\n\t\tself.cache_memory = cache_memory\n\t\tself.source_files = {}\n\t\tself.target_files = {}\n\n\tdef __len__(self):\n\t\treturn self.img_num\n\t\n\tdef __getitem__(self, idx):\n\t\tcv2.setNumThreads(0)\n\t\tcv2.ocl.setUseOpenCL(False)\n\t\timg_name = self.img_names[idx]\n\n    \t# Try to load from cache if cache_memory is enabled\n\t\tif self.cache_memory and img_name in self.source_files:\n\t\t\tsource_img = self.source_files[img_name]\n\t\t\ttarget_img = self.target_files[img_name]\n\t\telse:\n        \t# Fallback: Load from disk if not cached\n\t\t\tsource_img = read_img(os.path.join(self.root_dir, 'IN', img_name), to_float=False)\n\t\t\ttarget_img = read_img(os.path.join(self.root_dir, 'GT', img_name), to_float=False)\n\n        \t# Cache the images in memory if cache_memory is enabled\n\t\t\tif self.cache_memory:\n\t\t\t\tself.source_files[img_name] = source_img\n\t\t\t\tself.target_files[img_name] = target_img\n\n    \t# Convert to [-1, 1] range\n\t\tsource_img = source_img.astype('float32') / 255.0 * 2 - 1\n\t\ttarget_img = target_img.astype('float32') / 255.0 * 2 - 1\n\n    \t# Apply data augmentation or alignment\n\t\tif self.mode == 'train':\n\t\t\t[source_img, target_img] = augment([source_img, target_img], self.size, self.edge_decay, self.data_augment)\n\t\telif self.mode == 'valid':\n\t\t\t[source_img, target_img] = align([source_img, target_img], self.size)\n\n    \t# Convert to PyTorch tensors\n\t\tsource_tensor = torch.tensor(hwc_to_chw(source_img), dtype=torch.float32)\n\t\ttarget_tensor = torch.tensor(hwc_to_chw(target_img), dtype=torch.float32)\n\t\treturn {'source': source_tensor, 'target': target_tensor, 'filename': img_name}\n\n#----------------------------------------------------------------------------------------------------------------------------------\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:15.777081Z","iopub.execute_input":"2025-05-25T11:11:15.777359Z","iopub.status.idle":"2025-05-25T11:11:15.790813Z","shell.execute_reply.started":"2025-05-25T11:11:15.777334Z","shell.execute_reply":"2025-05-25T11:11:15.790059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#                                    ------------GUNET.PY FOLDER-------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.nn.init import _calculate_fan_in_and_fan_out\nfrom timm.models.layers import to_2tuple, trunc_normal_\n#from .norm_layer import *\n\n\nclass ConvLayer(nn.Module):\n\tdef __init__(self, net_depth, dim, kernel_size=3, gate_act=nn.Sigmoid):\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\n\t\tself.net_depth = net_depth\n\t\tself.kernel_size = kernel_size\n\n\t\tself.Wv = nn.Sequential(\n\t\t\tnn.Conv2d(dim, dim, 1),\n\t\t\tnn.Conv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size//2, groups=dim, padding_mode='reflect')\n\t\t)\n\n\t\tself.Wg = nn.Sequential(\n\t\t\tnn.Conv2d(dim, dim, 1),\n\t\t\tgate_act() if gate_act in [nn.Sigmoid, nn.Tanh] else gate_act(inplace=True)\n\t\t)\n\n\t\tself.proj = nn.Conv2d(dim, dim, 1)\n\n\t\tself.apply(self._init_weights)\n\n\tdef _init_weights(self, m):\n\t\tif isinstance(m, nn.Conv2d):\n\t\t\tgain = (8 * self.net_depth) ** (-1/4)    # self.net_depth ** (-1/2), the deviation seems to be too small, a bigger one may be better\n\t\t\tfan_in, fan_out = _calculate_fan_in_and_fan_out(m.weight)\n\t\t\tstd = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n\t\t\ttrunc_normal_(m.weight, std=std)\n\n\t\t\tif m.bias is not None:\n\t\t\t\tnn.init.constant_(m.bias, 0)\n\n\tdef forward(self, X):\n\t\tout = self.Wv(X) * self.Wg(X)\n\t\tout = self.proj(out)\n\t\treturn out\n\n\nclass BasicBlock(nn.Module):\n\tdef __init__(self, net_depth, dim, kernel_size=3, conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid):\n\t\tsuper().__init__()\n\t\tself.norm = norm_layer(dim)\n\t\tself.conv = conv_layer(net_depth, dim, kernel_size, gate_act)\n\tdef forward(self, x):\n\t\tidentity = x\n\t\tx = self.norm(x)\n\t\tx = self.conv(x)\n\t\tx = identity + x\n\t\treturn x\n\n\nclass BasicLayer(nn.Module):\n\tdef __init__(self, net_depth, dim, depth, kernel_size=3, conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid):\n\n\t\tsuper().__init__()\n\t\tself.dim = dim\n\t\tself.depth = depth\n\n\t\t# build blocks\n\t\tself.blocks = nn.ModuleList([\n\t\t\tBasicBlock(net_depth, dim, kernel_size, conv_layer, norm_layer, gate_act)\n\t\t\tfor i in range(depth)])\n\n\tdef forward(self, x):\n\t\tfor blk in self.blocks:\n\t\t\tx = blk(x)\n\t\treturn x\n\n\nclass PatchEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, in_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.in_chans = in_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = patch_size\n\n\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=patch_size,\n\t\t\t\t\t\t\t  padding=(kernel_size-patch_size+1)//2, padding_mode='reflect')\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass PatchUnEmbed(nn.Module):\n\tdef __init__(self, patch_size=4, out_chans=3, embed_dim=96, kernel_size=None):\n\t\tsuper().__init__()\n\t\tself.out_chans = out_chans\n\t\tself.embed_dim = embed_dim\n\n\t\tif kernel_size is None:\n\t\t\tkernel_size = 1\n\n\t\tself.proj = nn.Sequential(\n\t\t\tnn.Conv2d(embed_dim, out_chans*patch_size**2, kernel_size=kernel_size,\n\t\t\t\t\t  padding=kernel_size//2, padding_mode='reflect'),\n\t\t\tnn.PixelShuffle(patch_size)\n\t\t)\n\n\tdef forward(self, x):\n\t\tx = self.proj(x)\n\t\treturn x\n\n\nclass SKFusion(nn.Module):\n\tdef __init__(self, dim, height=2, reduction=8):\n\t\tsuper(SKFusion, self).__init__()\n\n\t\tself.height = height\n\t\td = max(int(dim/reduction), 4)\n\n\t\tself.mlp = nn.Sequential(\n\t\t\tnn.AdaptiveAvgPool2d(1),\n\t\t\tnn.Conv2d(dim, d, 1, bias=False),\n\t\t\tnn.ReLU(True),\n\t\t\tnn.Conv2d(d, dim*height, 1, bias=False)\n\t\t)\n\n\t\tself.softmax = nn.Softmax(dim=1)\n\n\tdef forward(self, in_feats):\n\t\tB, C, H, W = in_feats[0].shape\n\n\t\tin_feats = torch.cat(in_feats, dim=1)\n\t\tin_feats = in_feats.view(B, self.height, C, H, W)\n\n\t\tfeats_sum = torch.sum(in_feats, dim=1)\n\t\tattn = self.mlp(feats_sum)\n\t\tattn = self.softmax(attn.view(B, self.height, C, 1, 1))\n\n\t\tout = torch.sum(in_feats*attn, dim=1)\n\t\treturn out\n\n\nclass gUNet(nn.Module):\n\tdef __init__(self, kernel_size=5, base_dim=32, depths=[4, 4, 4, 4, 4, 4, 4], conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid, fusion_layer=SKFusion):\n\t\tsuper(gUNet, self).__init__()\n\t\t# setting\n\t\tassert len(depths) % 2 == 1\n\t\tstage_num = len(depths)\n\t\thalf_num = stage_num // 2\n\t\tnet_depth = sum(depths)\n\t\tembed_dims = [2**i*base_dim for i in range(half_num)]\n\t\tembed_dims = embed_dims + [2**half_num*base_dim] + embed_dims[::-1]\n\n\t\tself.patch_size = 2 ** (stage_num // 2)\n\t\tself.stage_num = stage_num\n\t\tself.half_num = half_num\n\n\t\t# input convolution\n\t\tself.inconv = PatchEmbed(patch_size=1, in_chans=3, embed_dim=embed_dims[0], kernel_size=3)\n\n\t\t# backbone\n\t\tself.layers = nn.ModuleList()\n\t\tself.downs = nn.ModuleList()\n\t\tself.ups = nn.ModuleList()\n\t\tself.skips = nn.ModuleList()\n\t\tself.fusions = nn.ModuleList()\n\n\t\tfor i in range(self.stage_num):\n\t\t\tself.layers.append(BasicLayer(dim=embed_dims[i], depth=depths[i], net_depth=net_depth, kernel_size=kernel_size, \n\t\t\t\t\t\t\t\t\t\t  conv_layer=conv_layer, norm_layer=norm_layer, gate_act=gate_act))\n\n\t\tfor i in range(self.half_num):\n\t\t\tself.downs.append(PatchEmbed(patch_size=2, in_chans=embed_dims[i], embed_dim=embed_dims[i+1]))\n\t\t\tself.ups.append(PatchUnEmbed(patch_size=2, out_chans=embed_dims[i], embed_dim=embed_dims[i+1]))\n\t\t\tself.skips.append(nn.Conv2d(embed_dims[i], embed_dims[i], 1))\n\t\t\tself.fusions.append(fusion_layer(embed_dims[i]))\n\n\t\t# output convolution\n\t\tself.outconv = PatchUnEmbed(patch_size=1, out_chans=3, embed_dim=embed_dims[-1], kernel_size=3)\n\n\n\tdef forward(self, x):\n\t\tfeat = self.inconv(x)\n\n\t\tskips = []\n\n\t\tfor i in range(self.half_num):\n\t\t\tfeat = self.layers[i](feat)\n\t\t\tskips.append(self.skips[i](feat))\n\t\t\tfeat = self.downs[i](feat)\n\n\t\tfeat = self.layers[self.half_num](feat)\n\n\t\tfor i in range(self.half_num-1, -1, -1):\n\t\t\tfeat = self.ups[i](feat)\n\t\t\tfeat = self.fusions[i]([feat, skips[i]])\n\t\t\tfeat = self.layers[self.stage_num-i-1](feat)\n\n\t\tx = self.outconv(feat) + x\n\n\t\treturn x\n\n\n__all__ = ['gUNet', 'gunet_t', 'gunet_s', 'gunet_b', 'gunet_d']\n\n# Normalization batch size of 16~32 may be good\n\ndef gunet_t():\t# 4 cards 2080Ti\n\treturn gUNet(kernel_size=5, base_dim=24, depths=[2, 2, 2, 4, 2, 2, 2], conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid, fusion_layer=SKFusion)\n\ndef gunet_s():\t# 4 cards 3090\n\treturn gUNet(kernel_size=5, base_dim=24, depths=[4, 4, 4, 8, 4, 4, 4], conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid, fusion_layer=SKFusion)\n\ndef gunet_b():\t# 4 cards 3090\n\treturn gUNet(kernel_size=5, base_dim=24, depths=[8, 8, 8, 16, 8, 8, 8], conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid, fusion_layer=SKFusion)\n\ndef gunet_d():\t# 4 cards 3090\n\treturn gUNet(kernel_size=5, base_dim=24, depths=[16, 16, 16, 32, 16, 16, 16], conv_layer=ConvLayer, norm_layer=nn.BatchNorm2d, gate_act=nn.Sigmoid, fusion_layer=SKFusion)\n\n\n\n\n\n\ndef convert_model(module):\n    \"\"\"Traverse the input module and its child recursively\n       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n       to SynchronizedBatchNorm*N*d\n\n    Args:\n        module: the input module needs to be convert to SyncBN model\n\n    Examples:\n        >>> import torch.nn as nn\n        >>> import torchvision\n        >>> # m is a standard pytorch model\n        >>> m = torchvision.models.resnet18(True)\n        >>> m = nn.DataParallel(m)\n        >>> # after convert, m is using SyncBN\n        >>> m = convert_model(m)\n    \"\"\"\n    if isinstance(module, torch.nn.DataParallel):\n        mod = module.module\n        mod = convert_model(mod)\n        mod.cuda()\n        mod = DataParallelWithCallback(mod, device_ids=module.device_ids)\n        return mod\n\n    mod = module\n    for pth_module, sync_module in zip([torch.nn.BatchNorm1d,\n                                        torch.nn.BatchNorm2d,\n                                        torch.nn.BatchNorm3d],\n                                       [SynchronizedBatchNorm1d,\n                                        SynchronizedBatchNorm2d,\n                                        SynchronizedBatchNorm3d]):\n        if isinstance(module, pth_module):\n            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)\n            mod.running_mean = module.running_mean\n            mod.running_var = module.running_var\n            if module.affine:\n                mod.weight.data = module.weight.data.clone().detach()\n                mod.bias.data = module.bias.data.clone().detach()\n\n    for name, child in module.named_children():\n        mod.add_module(name, convert_model(child))\n\n    return mod\n\n\nclass DataParallelWithCallback(DataParallel):\n    \"\"\"\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    \"\"\"\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\t\n\ndef execute_replication_callbacks(modules):\n    \"\"\"\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    \"\"\"\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, '__data_parallel_replicate__'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\nclass CallbackContext(object):\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:15.792210Z","iopub.execute_input":"2025-05-25T11:11:15.792833Z","iopub.status.idle":"2025-05-25T11:11:15.956197Z","shell.execute_reply.started":"2025-05-25T11:11:15.792809Z","shell.execute_reply":"2025-05-25T11:11:15.955347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#                                               BATCHNORM.PY IN SYNC_BATCHNORM FOLDER\n\nimport collections\nimport contextlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\ntry:\n    from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\nexcept ImportError:\n    ReduceAddCoalesced = Broadcast = None\n\n\n\n__all__ = [\n    'set_sbn_eps_mode',\n    'SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d',\n    'patch_sync_batchnorm', 'convert_model'\n]\n\n\nSBN_EPS_MODE = 'clamp'\n\n\ndef set_sbn_eps_mode(mode):\n    global SBN_EPS_MODE\n    assert mode in ('clamp', 'plus')\n    SBN_EPS_MODE = mode\n\n\ndef _sum_ft(tensor):\n    \"\"\"sum over the first and last dimention\"\"\"\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    \"\"\"add new dimensions at the front and the tail\"\"\"\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n\n\n\n\n\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n        assert ReduceAddCoalesced is not None, 'Can not use Synchronized Batch Normalization without CUDA support.'\n\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine,\n                                                     track_running_stats=track_running_stats)\n\n        if not self.track_running_stats:\n            import warnings\n            warnings.warn('track_running_stats=False is not supported by the SynchronizedBatchNorm.')\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        assert input.size(1) == self.num_features, 'Channel size mismatch: got {}, expect {}.'.format(input.size(1), self.num_features)\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n\n        # Always using same \"device order\" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.\"\"\"\n        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        if hasattr(torch, 'no_grad'):\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n        else:\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        if SBN_EPS_MODE == 'clamp':\n            return mean, bias_var.clamp(self.eps) ** -0.5\n        elif SBN_EPS_MODE == 'plus':\n            return mean, (bias_var + self.eps) ** -0.5\n        else:\n            raise ValueError('Unknown EPS mode: {}.'.format(SBN_EPS_MODE))\n\n\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n\t\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError('expected 2D or 3D input (got {}D input)'\n                             .format(input.dim()))\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError('expected 4D input (got {}D input)'\n                             .format(input.dim()))\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError('expected 5D input (got {}D input)'\n                             .format(input.dim()))\n\n\n\n#                                                COMM.PY IN SYNC_BATCHNORM FOLDER\n\n\nimport queue\nimport collections\nimport threading\n\n__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n\n\nclass FutureResult(object):\n    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, 'Previous result has\\'t been fetched.'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n_SlavePipeBase = collections.namedtuple('_SlavePipeBase', ['identifier', 'queue', 'result'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    \"\"\"Pipe for master-slave communication.\"\"\"\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    \"\"\"An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    \"\"\"\n\n    def __init__(self, master_callback):\n        \"\"\"\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        \"\"\"\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {'master_callback': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state['master_callback'])\n\n    def register_slave(self, identifier):\n        \"\"\"\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        \"\"\"\n        if self._activated:\n            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        \"\"\"\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        \"\"\"\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, 'The first result should belongs to the master.'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n\n#----------------------------------------------------------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------------------------------------------------------\n#----------------------------------------------------------------------------------------------------------------------------------\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:15.957018Z","iopub.execute_input":"2025-05-25T11:11:15.957255Z","iopub.status.idle":"2025-05-25T11:11:15.981945Z","shell.execute_reply.started":"2025-05-25T11:11:15.957237Z","shell.execute_reply":"2025-05-25T11:11:15.981427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GPU Config\n# Automatically use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Device Count:\", torch.cuda.device_count())\nprint(\"Current Device:\", torch.cuda.current_device())\nprint(\"Device Name:\", torch.cuda.get_device_name(0))\nprint(\"PyTorch CUDA version:\", torch.version.cuda)\n\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'\n\ntorch.cuda.empty_cache()\ntorch.cuda.ipc_collect()\n\nimport argparse\nimport sys\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--model', default='gunet_t', type=str, help='model name')\nparser.add_argument('--num_workers', default=16, type=int, help='number of workers')\nparser.add_argument('--use_mp', action='store_true', default=False, help='use Mixed Precision')\nparser.add_argument('--use_ddp', action='store_true', default=False, help='use Distributed Data Parallel')\n#parser.add_argument('--save_dir', default='./saved_models/', type=str, help='path to models saving')\nparser.add_argument('--save_dir', default='/kaggle/working/saved_models/', type=str, help='path to models saving')\n#parser.add_argument('--data_dir', default='./data/', type=str, help='path to dataset')   # Keep the base data path\nparser.add_argument('--data_dir', default='/kaggle/input/', type=str, help='path to dataset')\nparser.add_argument('--log_dir', default='./logs/', type=str, help='path to logs')\n\n# Update dataset names to match your folder structure\nparser.add_argument('--train_set', default='haze4k-t/Haze4K-T', type=str, help='train dataset name')   # Train dataset\nparser.add_argument('--val_set', default='haze4k-v/Haze4K-V', type=str, help='valid dataset name')     # Validation dataset\n\n# Experiment settings\nparser.add_argument('--exp', default='reside-in', type=str, help='experiment setting')\n\n#args = parser.parse_args()\nargs, _ = parser.parse_known_args()\n\n# training environment\nif args.use_ddp:\n\ttorch.distributed.init_process_group(backend='nccl', init_method='env://')\n\tworld_size = dist.get_world_size()\n\tlocal_rank = dist.get_rank()\n\ttorch.cuda.set_device(local_rank)\n\tif local_rank == 0: print('==> Using DDP.')\nelse:\n\tworld_size = 1\n\n\n# training config\n\nb_setup = {\n    \"t_patch_size\": 256,             \n    \"valid_mode\": \"valid\",\n    \"v_patch_size\": 400,             \n    \"v_batch_ratio\": 1.0,\n    \"edge_decay\": 0.1,\n    \"weight_decay\": 0.01,\n    \"data_augment\": True,\n    \"cache_memory\": False,\n    \"num_iter\": 16384,                \n    \"epochs\": 1000,                   \n    \"warmup_epochs\": 50,             \n    \"const_epochs\": 0,\n    \"frozen_epochs\": 200,\n    \"eval_freq\": 10                    \n}\n\nvariant = args.model.split('_')[-1]\nconfig_name = 'model_'+variant+'.json' if variant in ['t', 's', 'b', 'd'] else 'default.json'\t# default.json as baselines' configuration file \n\nm_setup = {\n\"batch_size\": 2,\n\"lr\": 8e-4\n}\n\n\ndef reduce_mean(tensor, nprocs):\n\trt = tensor.clone()\n\tdist.all_reduce(rt, op=dist.ReduceOp.SUM)\n\trt /= nprocs\n\treturn rt\n\n\ndef train(train_loader, network, criterion, optimizer, scaler, epochs, frozen_bn=False, plot_interval=50):\n\tlosses = AverageMeter()\n\n\ttorch.cuda.empty_cache()\n\t\n\tnetwork.eval() if frozen_bn else network.train()\t# simplified implementation that other modules may be affected\n\t\n\n#\tfor batch in enumerate(train_loader):\n#\t\tsource_img = batch['source'].cuda()\n#\t\ttarget_img = batch['target'].cuda()\n\tbatch_count = 0  # ✅ Counter to track batch index\n\tfor batch in train_loader:\n\t\tbatch_count += 1  # ✅ Increment batch counter manually\n\n\t\t# ✅ Access images from dictionary\n\t\tsource_img = batch['source'].to('cuda')\n\t\ttarget_img = batch['target'].to('cuda')\n\t\toutput = network(source_img)\n\n\n\t\twith autocast(args.use_mp):\n\t\t\toutput = network(source_img)\n\t\t\tloss = criterion(output, target_img)\n\n\t\toptimizer.zero_grad()\n\t\tscaler.scale(loss).backward()\n\t\tscaler.step(optimizer)\n\t\tscaler.update()\n\n\t\tif args.use_ddp: loss = reduce_mean(loss, dist.get_world_size())\n\t\tlosses.update(loss.item())\n\n#\t\t# Show images during training\n#\t\tif batch % 10 == 0:\n#\t\t\tshow_images(source_img, target_img, output, epochs, batch)\n\n        # ✅ Automatically plot and save images every `plot_interval` batches\n\t\tif batch_count % plot_interval == 0:\n\t\t\t#print(f\"\\n🛠️ Plotting images at Epoch {epochs}, Batch {batch_count}...\")\n\t\t\tshow_images(source_img, target_img, output, epochs, batch_count)\n\n\n\n\treturn losses.avg\n\n\n\n# New Validation loss function\n\nfrom skimage.metrics import structural_similarity as compare_ssim\nimport torch.nn.functional as F\n'''\ndef valid(val_loader, network, criterion):\n    \"\"\"Returns (avg_psnr, avg_ssim, avg_val_loss) over val_loader.\"\"\"\n    psnr_meter = AverageMeter()\n    loss_meter = AverageMeter()\n    ssim_meter = AverageMeter()\n\n    torch.cuda.empty_cache()\n    network.eval()\n\n    for batch in val_loader:\n        src = batch['source'].to('cuda')\n        tgt = batch['target'].to('cuda')\n\n        with torch.no_grad():\n            H, W = src.shape[2:]\n            pad = (network.module.patch_size\n                   if hasattr(network.module, 'patch_size')\n                   else 16)\n            src_p = pad_img(src, pad)\n            out = network(src_p).clamp_(-1, 1)\n            out = out[:, :, :H, :W]\n\n        # compute loss\n        loss = criterion(out, tgt).item()\n        loss_meter.update(loss, src.size(0))\n\n        # compute PSNR\n        mse_per_image = F.mse_loss(out * 0.5 + 0.5,\n                                   tgt * 0.5 + 0.5,\n                                   reduction='none') \\\n                         .mean((1,2,3))\n        psnr_batch = (10 * torch.log10(1 / mse_per_image)).mean().item()\n        psnr_meter.update(psnr_batch, src.size(0))\n\n        # compute SSIM (per-image, on CPU numpy)\n        out_np = (out * 0.5 + 0.5).cpu().numpy()\n        tgt_np = (tgt * 0.5 + 0.5).cpu().numpy()\n        # loop over batch\n        ssim_sum = 0\n        for i in range(out_np.shape[0]):\n            # assume channels-first RGB or grayscale\n            # for multichannel=True if C>1\n            s = compare_ssim(out_np[i].transpose(1,2,0),\n                             tgt_np[i].transpose(1,2,0),\n                             data_range=1.0,\n                             multichannel=True)\n            ssim_sum += s\n        ssim_batch = ssim_sum / out_np.shape[0]\n        ssim_meter.update(ssim_batch, src.size(0))\n\n    return psnr_meter.avg, ssim_meter.avg, loss_meter.avg\n\n'''\ndef valid(val_loader, network, criterion):\n    \"\"\"Returns (avg_psnr, avg_ssim, avg_val_loss) over val_loader.\"\"\"\n    psnr_meter = AverageMeter()\n    loss_meter = AverageMeter()\n    ssim_meter = AverageMeter()\n\n    torch.cuda.empty_cache()\n    network.eval()\n\n    for batch in val_loader:\n        src = batch['source'].to('cuda')\n        tgt = batch['target'].to('cuda')\n\n        with torch.no_grad():\n            H, W = src.shape[2:]\n            pad = (network.module.patch_size\n                   if hasattr(network.module, 'patch_size')\n                   else 16)\n            src_p = pad_img(src, pad)\n            out = network(src_p).clamp_(-1, 1)\n            out = out[:, :, :H, :W]\n\n        # 1) compute and record L1 loss\n        val_loss = criterion(out, tgt).item()\n        loss_meter.update(val_loss, src.size(0))\n\n        # 2) compute and record PSNR\n        mse_per_image = F.mse_loss(out * 0.5 + 0.5,\n                                   tgt * 0.5 + 0.5,\n                                   reduction='none') \\\n                         .mean((1, 2, 3))\n        psnr_batch = (10 * torch.log10(1 / mse_per_image)).mean().item()\n        psnr_meter.update(psnr_batch, src.size(0))\n\n        # 3) compute and record SSIM per image\n        out_np = (out * 0.5 + 0.5).cpu().numpy()  # shape: (N, C, H, W)\n        tgt_np = (tgt * 0.5 + 0.5).cpu().numpy()\n        batch_ssim = 0.0\n\n        for i in range(out_np.shape[0]):\n            img = out_np[i]\n            ref = tgt_np[i]\n            C, h, w = img.shape\n\n            # pick an odd window size between 3 and min(h, w)\n            win_size = min(h, w)\n            if win_size % 2 == 0:\n                win_size -= 1\n            win_size = max(win_size, 3)\n\n            # prepare HxW or HxWxC array\n            img_hw = img.transpose(1, 2, 0)\n            ref_hw = ref.transpose(1, 2, 0)\n\n            if C == 1:\n                # squeeze singleton channel for grayscale\n                img_hw = img_hw[:, :, 0]\n                ref_hw = ref_hw[:, :, 0]\n                channel_axis = None\n            else:\n                channel_axis = -1\n\n            s = compare_ssim(\n                img_hw,\n                ref_hw,\n                data_range=1.0,\n                win_size=win_size,\n                channel_axis=channel_axis\n            )\n            batch_ssim += s\n\n        ssim_batch = batch_ssim / out_np.shape[0]\n        ssim_meter.update(ssim_batch, src.size(0))\n\n    return psnr_meter.avg, ssim_meter.avg, loss_meter.avg\n\n\n\n\n\n\n\n# Visualization Function\n\n\ndef show_images(orig, target, output, epoch, batch, save_dir=\"./plots\", display_time=3):\n    \"\"\" \n    Visualize and save original, target, and output images side by side.\n    Automatically close and save the images.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n\n    plt.ion()  # ✅ Enable interactive mode\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n    # Convert tensors to NumPy and transpose for display\n    orig = orig.detach().cpu().numpy().transpose(0, 2, 3, 1)[0]\n    target = target.detach().cpu().numpy().transpose(0, 2, 3, 1)[0]\n    output = output.detach().cpu().numpy().transpose(0, 2, 3, 1)[0]\n\n    # Normalize and clip the values to [0,1] range\n    orig = (orig * 0.5 + 0.5).clip(0, 1)\n    target = (target * 0.5 + 0.5).clip(0, 1)\n    output = (output * 0.5 + 0.5).clip(0, 1)\n\n    # Display images\n    axes[0].imshow(orig)\n    axes[0].set_title('Original')\n    axes[0].axis('off')\n\n    axes[1].imshow(target)\n    axes[1].set_title('Ground Truth')\n    axes[1].axis('off')\n\n    axes[2].imshow(output)\n    axes[2].set_title('Output')\n    axes[2].axis('off')\n\n    plt.suptitle(f'Epoch: {epoch}, Batch: {batch}')\n\n    # ✅ Save the image\n    plot_path = os.path.join(save_dir, f\"epoch_{epoch}_batch_{batch}.png\")\n    plt.savefig(plot_path)\n    print(f\"✅ Saved plot: {plot_path}\")\n\n    # ✅ Display for `display_time` seconds, then close automatically\n    plt.pause(display_time)\n    plt.close(fig)  # ✅ Automatically close the plot\n    plt.ioff()  # Disable interactive mode\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:15.982686Z","iopub.execute_input":"2025-05-25T11:11:15.982936Z","iopub.status.idle":"2025-05-25T11:11:16.123759Z","shell.execute_reply.started":"2025-05-25T11:11:15.982896Z","shell.execute_reply":"2025-05-25T11:11:16.123191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_latest_checkpoint(model_name, save_dir):\n\tfiles = [f for f in os.listdir(save_dir) if f.startswith(model_name) and 'epoch' in f and f.endswith('.pth')]\n\tif not files:\n\t\treturn None\n\tfiles.sort(key=lambda f: int(re.findall(r'epoch(\\d+)', f)[0]))\n\treturn os.path.join(save_dir, files[-1])\n\ndef resume_or_initialize(model, optimizer, lr_scheduler, wd_scheduler, scaler, save_dir, model_name):\n\tcheckpoint_path = find_latest_checkpoint(model_name, save_dir)\n\tif checkpoint_path is None:\n\t\tprint(\"🆕 No checkpoint found. Starting fresh.\")\n\t\thistory = {'epoch': [], 'train_loss': [], 'val_loss': [], 'psnr': [], 'ssim': []}\n\t\treturn 0, 0, history\n\telse:\n\t\tprint(f\"🔁 Resuming from checkpoint: {checkpoint_path}\")\n\t\tcheckpoint = torch.load(checkpoint_path, map_location='cpu')\n\t\tmodel.load_state_dict(checkpoint['state_dict'])\n\t\toptimizer.load_state_dict(checkpoint['optimizer'])\n\t\tlr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n\t\twd_scheduler.load_state_dict(checkpoint['wd_scheduler'])\n\t\tscaler.load_state_dict(checkpoint['scaler'])\n\t\thistory = checkpoint.get('history', {'epoch': [], 'train_loss': [], 'val_loss': [], 'psnr': [], 'ssim': []})\n\t\treturn checkpoint['cur_epoch'], checkpoint['best_psnr'], history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:11:19.427278Z","iopub.execute_input":"2025-05-25T11:11:19.427760Z","iopub.status.idle":"2025-05-25T11:11:19.434330Z","shell.execute_reply.started":"2025-05-25T11:11:19.427737Z","shell.execute_reply":"2025-05-25T11:11:19.433598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    import os\n    import glob\n    import zipfile\n    import shutil\n    import pandas as pd\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.nn.parallel import DataParallel, DistributedDataParallel\n    from torch.utils.data import DataLoader, RandomSampler\n    from torch.utils.tensorboard import SummaryWriter\n    from torch.cuda.amp import GradScaler\n    from tqdm import tqdm\n\n    # 1) Set up save directory under Kaggle working\n    save_dir = '/kaggle/working/saved_models'\n    os.makedirs(save_dir, exist_ok=True)\n\n    # 2) Unzip any uploaded checkpoint zips into save_dir\n    for z in glob.glob('/kaggle/input/*.zip'):\n        with zipfile.ZipFile(z, 'r') as zip_ref:\n            zip_ref.extractall(save_dir)\n    # Also copy .pth checkpoints from all datasets (not zipped)\n    for root, dirs, files in os.walk('/kaggle/input/'):\n        for f in files:\n            if f.endswith('.pth'):\n                full_path = os.path.join(root, f)\n                print(\"Found checkpoint:\", full_path)\n                shutil.copy(full_path, save_dir)\n\n    # 3) History for CSV\n    history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'psnr': [], 'ssim': []}\n\n    # 4) Build model\n    network = eval(args.model)().cuda()\n    if args.use_ddp:\n        network = DistributedDataParallel(network, device_ids=[local_rank], output_device=local_rank)\n        if m_setup['batch_size'] // world_size < 16:\n            if local_rank == 0:\n                print('==> Using SyncBN (small batch per GPU)')\n            nn.SyncBatchNorm.convert_sync_batchnorm(network)\n    else:\n        network = DataParallel(network)\n        if m_setup['batch_size'] // torch.cuda.device_count() < 16:\n            print('==> Using SyncBN (small batch overall)')\n            convert_model(network)\n\n    # 5) Loss, optimizer, schedulers, scaler\n    criterion = nn.L1Loss()\n    optimizer = torch.optim.AdamW(network.parameters(), lr=m_setup['lr'],\n                                  weight_decay=b_setup['weight_decay'])\n    lr_scheduler = CosineScheduler(optimizer, param_name='lr',\n                                   t_max=b_setup['epochs'],\n                                   value_min=m_setup['lr'] * 1e-2,\n                                   warmup_t=b_setup['warmup_epochs'],\n                                   const_t=b_setup['const_epochs'])\n    wd_scheduler = CosineScheduler(optimizer, param_name='weight_decay',\n                                   t_max=b_setup['epochs'])\n    scaler = GradScaler()\n\n    # 6) Resume logic: find latest checkpoint\n\n    def latest_ckpt(save_dir, model_name):\n        snaps = sorted(\n            glob.glob(os.path.join(save_dir, f\"{model_name}_epoch*.pth\")),\n            key=lambda f: int(os.path.splitext(f)[0].split('epoch')[-1])\n        )\n        return snaps[-1] if snaps else None\n\n    ckpt = latest_ckpt(save_dir, args.model)\n    if ckpt:\n        print(f\"==> Resuming from checkpoint {ckpt}\")\n        #m = torch.load(ckpt, map_location='cpu')\n        m = torch.load(ckpt, map_location='cpu', weights_only=False)\n\n        network.load_state_dict(m['state_dict'])\n        optimizer.load_state_dict(m['optimizer'])\n        lr_scheduler.load_state_dict(m['lr_scheduler'])\n        wd_scheduler.load_state_dict(m['wd_scheduler'])\n        scaler.load_state_dict(m['scaler'])\n        cur_epoch = m.get('cur_epoch', 0)\n        best_psnr = m.get('best_psnr', 0)\n        history = m.get('history', history)\n    else:\n        print(\"==> No checkpoint found, starting fresh\")\n        cur_epoch = 0\n        best_psnr = 0\n    # 7) Data loaders\n    train_dataset = PairLoader(os.path.join(args.data_dir, args.train_set), 'train',\n                               b_setup['t_patch_size'], b_setup['edge_decay'],\n                               b_setup['data_augment'], b_setup['cache_memory'])\n    train_loader = DataLoader(train_dataset,\n                              batch_size=2,\n                              sampler=RandomSampler(train_dataset,\n                                                    num_samples=b_setup['num_iter']//world_size),\n                              num_workers=4, pin_memory=True,\n                              drop_last=True, persistent_workers=True)\n\n    val_dataset = PairLoader(os.path.join(args.data_dir, args.val_set),\n                             b_setup['valid_mode'], b_setup['v_patch_size'])\n    val_loader = DataLoader(val_dataset,\n                            batch_size=max(int(m_setup['batch_size'] *\n                                               b_setup['v_batch_ratio']//world_size), 1),\n                            num_workers=args.num_workers//world_size,\n                            pin_memory=True)\n\n    # 8) TensorBoard\n    if not args.use_ddp or local_rank == 0:\n        print('==> Start training:', args.model)\n        writer = SummaryWriter(log_dir=os.path.join(args.log_dir, args.exp, args.model))\n\n    # 9) Training loop\n    for epoch in tqdm(range(cur_epoch, b_setup['epochs']+1)):\n        frozen_bn = epoch > (b_setup['epochs'] - b_setup['frozen_epochs'])\n        # **Train**\n        loss = train(train_loader, network, criterion, optimizer, scaler, epoch, frozen_bn)\n        lr_scheduler.step(epoch+1)\n        wd_scheduler.step(epoch+1)\n\n        # record train loss\n        history['epoch'].append(epoch)\n        history['train_loss'].append(loss)\n        if not args.use_ddp or local_rank == 0:\n            writer.add_scalar('train_loss', loss, epoch)\n\n        # **Validate** every eval_freq\n        if epoch % b_setup['eval_freq'] == 0:\n            avg_psnr, avg_ssim, val_loss = valid(val_loader, network, criterion)\n            history['val_loss'].append(val_loss)\n            history['psnr'].append(avg_psnr)\n            history['ssim'].append(avg_ssim)\n\n            if not args.use_ddp or local_rank == 0:\n                # save best\n                if avg_psnr > best_psnr:\n                    best_psnr = avg_psnr\n                    torch.save({\n                        'cur_epoch': epoch+1,\n                        'best_psnr': best_psnr,\n                        'state_dict': network.state_dict(),\n                        'optimizer': optimizer.state_dict(),\n                        'lr_scheduler': lr_scheduler.state_dict(),\n                        'wd_scheduler': wd_scheduler.state_dict(),\n                        'scaler': scaler.state_dict(),\n                        'history': history\n                    }, os.path.join(save_dir, f\"{args.model}.pth\"))\n                    \n                    shutil.make_archive('/kaggle/working/saved_models', 'zip', '/kaggle/working/saved_models/')\n\n\n                # periodic snapshot\n                if epoch % 30 == 0:\n                    torch.save({\n                        'cur_epoch': epoch+1,\n                        'best_psnr': best_psnr,\n                        'state_dict': network.state_dict(),\n                        'optimizer': optimizer.state_dict(),\n                        'lr_scheduler': lr_scheduler.state_dict(),\n                        'wd_scheduler': wd_scheduler.state_dict(),\n                        'scaler': scaler.state_dict(),\n                        'history': history\n                    }, os.path.join(save_dir, f\"{args.model}_epoch{epoch}.pth\"))\n                    \n                    shutil.make_archive('/kaggle/working/saved_models', 'zip', '/kaggle/working/saved_models/')\n\n\n                writer.add_scalar('valid_psnr', avg_psnr, epoch)\n                writer.add_scalar('best_psnr',  best_psnr, epoch)\n            if args.use_ddp:\n                torch.distributed.barrier()\n        else:\n            # keep list alignment\n            history['val_loss'].append(None)\n            history['psnr'].append(None)\n            history['ssim'].append(None)\n\n        # **Dump CSV** every epoch\n        if not args.use_ddp or local_rank == 0:\n            df = pd.DataFrame(history)\n            df.to_csv(os.path.join(save_dir, f\"{args.model}_training_log.csv\"), index=False)\n\n    # 10) Done\n    if not args.use_ddp or local_rank == 0:\n        print(\"Done. Training log & checkpoints saved.\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:26:29.077411Z","iopub.execute_input":"2025-05-25T11:26:29.078079Z","execution_failed":"2025-05-25T11:38:29.006Z"}},"outputs":[],"execution_count":null}]}